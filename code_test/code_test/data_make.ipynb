{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import h5netcdf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset from .dat files\n",
    "# === setting ===\n",
    "BASE_PATH = \"./data/\"\n",
    "INPUT_PATHS = {\n",
    "    \"up_x\": os.path.join(BASE_PATH, \"fermi_line_kx_ky_Xpoint_up/\"),\n",
    "    \"up_ganma\": os.path.join(BASE_PATH, \"fermi_line_kx_ky_ganma_up/\"),\n",
    "    \"down_ganma\": os.path.join(BASE_PATH, \"fermi_line_kx_ky_ganma_down/\")\n",
    "}\n",
    "BZ_FILE_PATH = os.path.join(BASE_PATH, \"bz_line_ganma.dat\")\n",
    "OUTPUT_PATHS = {\n",
    "    \"up\": os.path.join(BASE_PATH, \"save_data/fermi_line_kx_ky_result_up/\"),\n",
    "    \"down\": os.path.join(BASE_PATH, \"save_data/fermi_line_kx_ky_result_down/\")\n",
    "}\n",
    "ROTATION_ANGLE = 45  # rotation (deg.)\n",
    "\n",
    "# === maximum value of BZ===\n",
    "bz_data = np.loadtxt(BZ_FILE_PATH)\n",
    "TRANSLATION_AMOUNT = np.max(np.abs(bz_data))\n",
    "\n",
    "# === functions ===\n",
    "def load_data(file_path):\n",
    "    \"\"\"loading dataset. arg=path\"\"\"\n",
    "    try:\n",
    "        data = np.loadtxt(file_path)\n",
    "        return data if data.size else np.empty((0, 2))\n",
    "    except (OSError, ValueError):\n",
    "        return np.empty((0, 2))\n",
    "\n",
    "def parallel_translate(coords, translation_vector):\n",
    "    return coords + translation_vector\n",
    "\n",
    "def rotate_coordinates(coords, angle_degrees):\n",
    "    angle_radians = np.radians(angle_degrees)\n",
    "    rotation_matrix = np.array([[np.cos(angle_radians), -np.sin(angle_radians)],\n",
    "                                [np.sin(angle_radians), np.cos(angle_radians)]])\n",
    "    return np.dot(coords, rotation_matrix)\n",
    "\n",
    "def process_and_save_data(filename):\n",
    "    \"\"\"save data for each files\"\"\"\n",
    "    data_up_X = load_data(os.path.join(INPUT_PATHS[\"up_x\"], filename))\n",
    "    data_up_ganma = load_data(os.path.join(INPUT_PATHS[\"up_ganma\"], filename))\n",
    "    data_down_ganma = load_data(os.path.join(INPUT_PATHS[\"down_ganma\"], filename))\n",
    "\n",
    "    translations = [\n",
    "        np.array([TRANSLATION_AMOUNT * 2, 0]),\n",
    "        np.array([-TRANSLATION_AMOUNT * 2, 0]),\n",
    "        np.array([0, TRANSLATION_AMOUNT * 2]),\n",
    "        np.array([0, -TRANSLATION_AMOUNT * 2]),\n",
    "        np.array([TRANSLATION_AMOUNT, TRANSLATION_AMOUNT]),\n",
    "        np.array([-TRANSLATION_AMOUNT, TRANSLATION_AMOUNT]),\n",
    "        np.array([TRANSLATION_AMOUNT, -TRANSLATION_AMOUNT]),\n",
    "        np.array([-TRANSLATION_AMOUNT, -TRANSLATION_AMOUNT])\n",
    "    ]\n",
    "\n",
    "    translated_coords_up = [parallel_translate(data_up_X, t) for t in translations[:4]] if data_up_X.size else []\n",
    "    translated_coords_X_up = [parallel_translate(data_up_ganma, t) for t in translations[4:]] if data_up_ganma.size else []\n",
    "    all_translated_coords_up = np.vstack([data_up_X] + translated_coords_up + translated_coords_X_up) if data_up_X.size or data_up_ganma.size else np.empty((0, 2))\n",
    "    rotated_coords_up = rotate_coordinates(all_translated_coords_up, ROTATION_ANGLE)\n",
    "\n",
    "    translated_coords_X_down = [parallel_translate(data_down_ganma, t) for t in translations[4:]] if data_down_ganma.size else []\n",
    "    all_translated_coords_down = np.vstack(translated_coords_X_down) if data_down_ganma.size else np.empty((0, 2))\n",
    "    rotated_coords_down = rotate_coordinates(all_translated_coords_down, ROTATION_ANGLE)\n",
    "    \n",
    "    os.makedirs(os.path.join(OUTPUT_PATHS[\"up\"]), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_PATHS[\"down\"]), exist_ok=True)\n",
    "\n",
    "    np.save(os.path.join(OUTPUT_PATHS[\"up\"], filename.replace(\".dat\", \".npy\")), rotated_coords_up)\n",
    "    np.save(os.path.join(OUTPUT_PATHS[\"down\"], filename.replace(\".dat\", \".npy\")), rotated_coords_down)\n",
    "\n",
    "# === main ===\n",
    "# composition_list = os.listdir(INPUT_PATHS[\"up_x\"])\n",
    "# for filename in composition_list:\n",
    "#     process_and_save_data(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image processing (mimicking SARPES data)\n",
    "# --- setting ---\n",
    "SIGMA = 4  # broadening width\n",
    "PIXEL_SIZE = 255  # image sizes\n",
    "BZ_MAX = TRANSLATION_AMOUNT  \n",
    "RANGE_PIXEL = (BZ_MAX / 2 * 5) / math.sqrt(2)  # calc. region\n",
    "\n",
    "# path setting\n",
    "DATA_DIRS = {\n",
    "    \"input_up\": os.path.join(BASE_PATH, \"save_data/fermi_line_kx_ky_result_up\"),\n",
    "    \"input_down\": os.path.join(BASE_PATH, \"save_data/fermi_line_kx_ky_result_down\"),\n",
    "    \"position_up\": os.path.join(BASE_PATH, \"save_data/data_position_list/kx_ky_up\"),\n",
    "    \"position_down\": os.path.join(BASE_PATH, \"save_data/data_position_list/kx_ky_down\"),\n",
    "    \"figure\": os.path.join(BASE_PATH, \"save_data/figure/fermi_line_kxky_fig\"),\n",
    "    \"gaussian_figure\": os.path.join(BASE_PATH, \"save_data/figure/gaussian\"),\n",
    "    \"gaussian_up\": os.path.join(BASE_PATH, f\"save_data/gaussian/sigma{SIGMA}_up_npy_kx_ky\"),\n",
    "    \"gaussian_down\": os.path.join(BASE_PATH, f\"save_data/gaussian/sigma{SIGMA}_down_npy_kx_ky\"),\n",
    "}\n",
    "\n",
    "# --- functions ---\n",
    "def load_npy(file_path):\n",
    "    \"\"\" loading npy data. if it don't exists, return empty.\"\"\"\n",
    "    return np.load(file_path) if os.path.exists(file_path) else np.empty((0, 2))\n",
    "\n",
    "def coordinates_to_pixels(data, x_min, x_max, y_min, y_max, pixel_size):\n",
    "    \"\"\" coordination to pixels \"\"\"\n",
    "    if data.size == 0:\n",
    "        return np.array([]).reshape(0, 2)\n",
    "    \n",
    "    pixel_x = ((data[:, 0] - x_min) / (x_max - x_min) * pixel_size).astype(int)\n",
    "    pixel_y = ((data[:, 1] - y_min) / (y_max - y_min) * pixel_size).astype(int)\n",
    "    return np.column_stack((pixel_x, pixel_y))\n",
    "\n",
    "def generate_smooth_gradient(size, white_pixel_positions, sigma):\n",
    "    \"\"\" broadening with gaussian blurring \"\"\"\n",
    "    image = np.zeros((size, size))\n",
    "    x, y = np.meshgrid(np.arange(size), np.arange(size))\n",
    "    \n",
    "    for cx, cy in white_pixel_positions:\n",
    "        d = np.sqrt((x - cx) ** 2 + (y - cy) ** 2)\n",
    "        image += np.exp(-d ** 2 / (2.0 * sigma ** 2))\n",
    "\n",
    "    # 画像の正規化\n",
    "    return (image - image.min()) / (image.max() - image.min()) if image.max() > 0 else image\n",
    "\n",
    "def process_files(filename):\n",
    "    \"\"\" main process. \"\"\"\n",
    "    filename = filename.replace(\".dat\", \"\")\n",
    "    \n",
    "    # データ読み込み\n",
    "    data_up = load_npy(os.path.join(DATA_DIRS[\"input_up\"], f\"{filename}.npy\"))\n",
    "    data_down = load_npy(os.path.join(DATA_DIRS[\"input_down\"], f\"{filename}.npy\"))\n",
    "\n",
    "    # 座標範囲設定\n",
    "    x_min, x_max, y_min, y_max = -RANGE_PIXEL, RANGE_PIXEL, -RANGE_PIXEL, RANGE_PIXEL\n",
    "\n",
    "    # ピクセル座標変換\n",
    "    pixel_coords_up = coordinates_to_pixels(data_up, x_min, x_max, y_min, y_max, PIXEL_SIZE)\n",
    "    pixel_coords_down = coordinates_to_pixels(data_down, x_min, x_max, y_min, y_max, PIXEL_SIZE)\n",
    "    \n",
    "    os.makedirs(os.path.join(DATA_DIRS[\"position_up\"]), exist_ok=True)\n",
    "    os.makedirs(os.path.join(DATA_DIRS[\"position_down\"]), exist_ok=True)\n",
    "    os.makedirs(os.path.join(DATA_DIRS[\"figure\"]), exist_ok=True)\n",
    "\n",
    "    # データ位置リストを保存\n",
    "    np.save(os.path.join(DATA_DIRS[\"position_up\"], f\"{filename}_up.npy\"), pixel_coords_up)\n",
    "    np.save(os.path.join(DATA_DIRS[\"position_down\"], f\"{filename}_down.npy\"), pixel_coords_down)\n",
    "\n",
    "    # --- プロット作成 ---\n",
    "    plt.scatter(pixel_coords_up[:, 0], pixel_coords_up[:, 1], color='red', label='up spin', s=5)\n",
    "    plt.scatter(pixel_coords_down[:, 0], pixel_coords_down[:, 1], color='blue', label='down spin', s=5)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(filename, fontsize=20)\n",
    "    plt.xlabel('kx', fontsize=20)\n",
    "    plt.ylabel('ky', fontsize=20)\n",
    "    plt.xlim(0, PIXEL_SIZE)\n",
    "    plt.ylim(PIXEL_SIZE, 0)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.savefig(os.path.join(DATA_DIRS[\"figure\"], f\"{filename}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # --- ガウシアン処理 ---\n",
    "    if pixel_coords_up.size > 0:\n",
    "        gradient_image_up = generate_smooth_gradient(PIXEL_SIZE, pixel_coords_up, SIGMA)\n",
    "    else:\n",
    "        gradient_image_up = np.zeros((PIXEL_SIZE, PIXEL_SIZE))\n",
    "\n",
    "    if pixel_coords_down.size > 0:\n",
    "        gradient_image_down = generate_smooth_gradient(PIXEL_SIZE, pixel_coords_down, SIGMA)\n",
    "    else:\n",
    "        gradient_image_down = np.zeros((PIXEL_SIZE, PIXEL_SIZE))\n",
    "        \n",
    "        \n",
    "    os.makedirs(os.path.join(DATA_DIRS[\"gaussian_up\"]), exist_ok=True)\n",
    "    os.makedirs(os.path.join(DATA_DIRS[\"gaussian_down\"]), exist_ok=True)\n",
    "    os.makedirs(os.path.join(DATA_DIRS[\"gaussian_figure\"]), exist_ok=True)\n",
    "\n",
    "    np.save(os.path.join(DATA_DIRS[\"gaussian_up\"], f\"{filename}_up.npy\"), gradient_image_up)\n",
    "    np.save(os.path.join(DATA_DIRS[\"gaussian_down\"], f\"{filename}_down.npy\"), gradient_image_down)\n",
    "\n",
    "    # --- スムージング後の画像表示 ---\n",
    "    combined_array = np.where(np.abs(gradient_image_up) > np.abs(gradient_image_down), gradient_image_up, -gradient_image_down)\n",
    "\n",
    "    plt.imshow(combined_array, cmap=\"seismic\", vmin=-1, vmax=1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(filename, fontsize=20)\n",
    "    plt.savefig(os.path.join(DATA_DIRS[\"gaussian_figure\"], f\"{filename}.png\"), bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# --- メイン処理 ---\n",
    "# for filename in tqdm(composition_list):\n",
    "#     process_files(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/save_data/h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定数の定義\n",
    "SIGMA = 4  # 必要に応じて変更\n",
    "INPUT_PATH_UP = DATA_DIRS[\"gaussian_up\"]\n",
    "INPUT_PATH_DOWN = DATA_DIRS[\"gaussian_down\"]\n",
    "OUTPUT_H5_PATH = os.path.join(BASE_PATH, \"save_data/h5\")\n",
    "\n",
    "\n",
    "# ガリウム組成のリストを生成\n",
    "Ga_composition = np.linspace(0, 100, 101, dtype=int) / 100\n",
    "\n",
    "# スピン偏極率データの読み込み\n",
    "spin_df = pd.read_csv(\"data/spin_polalization_article.csv\", header=0, names=[\"spin_polarization\"])\n",
    "\n",
    "# ファイルリストの取得（エラーハンドリング付き）\n",
    "def get_file_list(directory):\n",
    "    try:\n",
    "        return sorted(os.listdir(directory))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Directory not found - {directory}\")\n",
    "        return []\n",
    "\n",
    "gaussian_list_up = get_file_list(INPUT_PATH_UP)\n",
    "gaussian_list_down = get_file_list(INPUT_PATH_DOWN)\n",
    "\n",
    "# データの読み込み\n",
    "\n",
    "def load_gaussian_data(file_list, directory):\n",
    "    \"\"\"指定ディレクトリ内の NumPy ファイルを読み込んでリスト化\"\"\"\n",
    "    return [np.load(os.path.join(directory, filename)) for filename in file_list]\n",
    "\n",
    "up_data = load_gaussian_data(gaussian_list_up, INPUT_PATH_UP)\n",
    "down_data = load_gaussian_data(gaussian_list_down, INPUT_PATH_DOWN)\n",
    "\n",
    "# xarray Dataset の作成\n",
    "data = xr.Dataset(\n",
    "    {\n",
    "        \"up\": ([\"Ga_comp\", \"kx\", \"ky\"], up_data),\n",
    "        \"down\": ([\"Ga_comp\", \"kx\", \"ky\"], down_data),\n",
    "    },\n",
    "    coords={\n",
    "        \"Ga_comp\": Ga_composition,\n",
    "        \"kx\": np.arange(1, 256),\n",
    "        \"ky\": np.arange(1, 256),\n",
    "        \"spin_polarization\": np.array(spin_df[\"spin_polarization\"]),\n",
    "    },\n",
    "    attrs={\"material\": \"Co2MnGaGe\"},\n",
    ")\n",
    "\n",
    "\n",
    "# データの保存\n",
    "os.makedirs(OUTPUT_H5_PATH, exist_ok=True)\n",
    "data.to_netcdf(f\"{OUTPUT_H5_PATH}/data.h5\", mode=\"w\", engine=\"h5netcdf\")\n",
    "print(f\"Data saved to {OUTPUT_H5_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
